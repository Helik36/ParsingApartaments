import asyncio
import logging
import os
import random

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

from telegram.ext import ContextTypes

from check_in_db import append_urls, check_url_in_db

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

url = "https://www.avito.ru/syktyvkar/doma_dachi_kottedzhi/prodam-ASgBAgICAUSUA9AQ?cd=1&p=1&s=104&user=1"


async def pars_html():
    service = Service(executable_path="drive/chromedriver-linux64/chromedriver")
    # –ù—É–∂–Ω–æ —á—Ç–æ–±—ã –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –±–µ–∑ GUI. –°–∫—Ä–∏–Ω—à–æ—Ç—ã —Ç–∞–∫–∂–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")

    # –ù—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å, –ø–æ–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –≤—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, —Ç–∏–ø–∞ Js –∏–ª–∏ –µ—â—ë —á—Ç–æ-—Ç–æ.
    # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–∂–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
    # normal - Used by default, waits for all resources to download
    # eager	 - DOM access is ready, but other resources like images may still be loading
    # none   -Does not block WebDriver at all
    chrome_options.page_load_strategy = "eager"
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(options=chrome_options)

    # try:
    driver.get(url)
    assert "–ö—É–ø–∏—Ç—å" in driver.title

    html = driver.page_source

    try:
        os.mkdir("2section")
    except FileExistsError:
        pass

    with open(f"2section/doma_dachi_kottedzhi_page_1.html", "w", encoding='utf-8') as file:
        file.write(html)
    print(f"–°–∫–∞—á–∞–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—á–∞—Å—Ç–∫–æ–≤ - 1")

    with open(f"2section/doma_dachi_kottedzhi_page_1.html", "r", encoding='utf-8') as file:
        page = file.read()

    soup = BeautifulSoup(page, 'html.parser')

    last_page = soup.find_all("span", class_="styles-module-text-InivV")[-1].text

    for i in range(1, int(last_page)):

        try:
            check = driver.find_elements(By.CLASS_NAME, "styles-module-item-kF45w")[-1]
            check.click()

            html = driver.page_source

            print(f"–°–∫–∞—á–∞–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É —É—á–∞—Å—Ç–∫–æ–≤ - {i + 1}")

            with open(f"2section/doma_dachi_kottedzhi_page_{i + 1}.html", "w", encoding='utf-8') as file:
                file.write(html)
        except:
            print("–í–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–π —Ñ–∞–π–ª–∞")
            break

        if i == 1:
            break

    assert "No results found." not in driver.page_source

    driver.close()

    return int(2)
    # return int(last_page)

# except:
# print("–í–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞")
# driver.close()


# –°—é–¥–∞ –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∏–º–µ–Ω–Ω–æ –∫–∞–∫ (context: ContextTypes.DEFAULT_TYPE)
async def get_section_url(context: ContextTypes.DEFAULT_TYPE):
    pages = await pars_html()

    urls_from_db, date_from_db = await check_url_in_db("urls_section_db")
    for number_page in range(0, int(pages)):

        with open(f"2section/doma_dachi_kottedzhi_page_{number_page + 1}.html", "r", encoding='utf-8') as file:
            page = file.read()

        soup = BeautifulSoup(page, 'html.parser')

        new_urls = []
        for href in soup.find_all("a", class_="iva-item-sliderLink-uLz1v"):
            new_urls.append(f"https://www.avito.ru{href.get('href')}")

        for new_url in new_urls:
            if new_url not in urls_from_db:
                print(f"–î–æ–±–∞–≤–∏–ª —É—á–∞—Å—Ç–æ–∫ - {new_url}")
                await append_urls(new_url, "urls_section_db")
                await context.bot.send_message(chat_id=context.job.chat_id,
                                               text=f"üè° –ù–æ–≤–∞—è –¥–∞—á–∞/–¥–æ–º/–∫–æ—Ç—Ç–µ–¥–∂/—Ç–∞—É–Ω—Ö–∞—É—Å - {new_url}")
                await asyncio.sleep(2)

            else:
                pass


async def append_urls_in_db():
    pages = await pars_html()

    urls_from_db, date_from_db = await check_url_in_db("urls_section_db")
    for number_page in range(0, int(pages)):

        with open(f"2section/doma_dachi_kottedzhi_page_{number_page + 1}.html", "r", encoding='utf-8') as file:
            page = file.read()

        soup = BeautifulSoup(page, 'html.parser')

        new_urls = []
        for href in soup.find_all("a", class_="iva-item-sliderLink-uLz1v"):
            new_urls.append(f"https://www.avito.ru{href.get('href')}")

        for new_url in new_urls:
            if new_url not in urls_from_db:
                print(f"–î–æ–±–∞–≤–∏–ª - {new_url}")
                await append_urls(new_url, "urls_section_db")
            else:
                pass


if __name__ == '__main__':
    asyncio.run(append_urls_in_db())

import asyncio
import logging
import os
import random

from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

from telegram.ext import ContextTypes

from check_in_db import append_urls, check_url_in_db

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

url = "https://www.avito.ru/syktyvkar/kvartiry/prodam-ASgBAgICAUSSA8YQ?f=ASgBAgICAkSSA8YQkL4Nlq41&s=104"

just_text = ["–ù–æ–≤–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞, üèÉ‚Äç‚ôÇÔ∏è –±–µ–≥–æ–æ–æ–æ–æ–æ–æ–º", "–û–ø–∞-–æ–ø–∞-–æ–ø–∞ - –∫–≤–∞—Ä—Ç–∏—Ä–∞ üëæ!", "–ê–õ–Ø–†–ú üîî, –Ω–æ–≤–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞",
             "–¢–∞–º —ç—Ç–æ, –∫–≤–∞—Ä—Ç–∏—Ä–∞ –Ω–æ–≤–∞—è üëâüèª", "–ù—É —Ç—ã —Ç–∞–º –¥–æ–ª–≥–æ üò∂, –∑–∞–±–µ—Ä—É—Ç –≤–µ–¥—å —Å–∫–æ—Ä–æ"]


async def pars_html():
    service = Service(executable_path="drive/chromedriver")
    # –ù—É–∂–Ω–æ —á—Ç–æ–±—ã –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –±–µ–∑ GUI. –°–∫—Ä–∏–Ω—à–æ—Ç—ã —Ç–∞–∫–∂–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")

    # –ù—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å, –ø–æ–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –≤—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, —Ç–∏–ø–∞ Js –∏–ª–∏ –µ—â—ë —á—Ç–æ-—Ç–æ.
    # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–∂–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
    chrome_options.page_load_strategy = "eager"

    driver = webdriver.Chrome(options=chrome_options)

    driver.get(url)
    assert "–ö—É–ø–∏—Ç—å" in driver.title

    html = driver.page_source

    try:
        os.mkdir("2avito")
    except FileExistsError:
        pass

    with open(f"2avito/kvartiry_syktyvkar_page_1.html", "w", encoding='utf-8') as file:
        file.write(html)
    print(f"–°–∫–∞—á–∞–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É - 1")

    with open(f"2avito/kvartiry_syktyvkar_page_1.html", "r", encoding='utf-8') as file:
        page = file.read()

    soup = BeautifulSoup(page, 'html.parser')

    last_page = soup.find_all("span", class_="styles-module-text-InivV")[-1].text

    for i in range(1, int(last_page)):

        try:
            check = driver.find_elements(By.CLASS_NAME, "styles-module-item-kF45w")[-1]
            check.click()

            html = driver.page_source

            print(f"–°–∫–∞—á–∞–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É - {i + 1}")

            with open(f"2avito/kvartiry_syktyvkar_page_{i + 1}.html", "w", encoding='utf-8') as file:
                file.write(html)
        except:
            break

        if i == 1:
            break

    assert "No results found." not in driver.page_source

    driver.close()

    return int(2)


# –°—é–¥–∞ –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∏–º–µ–Ω–Ω–æ –∫–∞–∫ (context: ContextTypes.DEFAULT_TYPE)
async def get_url(context: ContextTypes.DEFAULT_TYPE):

    pages = await pars_html()
    for number_page in range(0, pages):

        with open(f"2avito/kvartiry_syktyvkar_page_{number_page + 1}.html", "r", encoding='utf-8') as file:
            page = file.read()

        soup = BeautifulSoup(page, 'html.parser')

        urls = []
        for href in soup.find_all("a", class_="iva-item-sliderLink-uLz1v"):
            urls.append(f"https://www.avito.ru{href.get('href')}")

        for url in urls:
            if url not in await check_url_in_db():
                print(f"–î–æ–±–∞–≤–∏–ª - {url}")
                await append_urls(url)
                random_text = random.choice(just_text)
                await context.bot.send_message(chat_id=context.job.chat_id, text=f"{random_text} - {url}")
                await asyncio.sleep(2)

            else:
                pass

# if __name__ == '__main__':
    # asyncio.run(pars_html())
    # asyncio.run(get_url())

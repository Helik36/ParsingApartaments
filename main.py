import asyncio
import os
import time
import random

from selenium.webdriver.common.by import By
from telegram import Update
from telegram.ext import Application, CommandHandler, ContextTypes
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service

from check_in_db import append_urls, check_url_in_db

url = "https://www.avito.ru/syktyvkar/kvartiry/prodam-ASgBAgICAUSSA8YQ?s=104"

just_text = ["–ù–æ–≤–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞, üèÉ‚Äç‚ôÇÔ∏è –±–µ–≥–æ–æ–æ–æ–æ–æ–æ–º", "–û–ø–∞-–æ–ø–∞-–æ–ø–∞ - –∫–≤–∞—Ä—Ç–∏—Ä–∞ üëæ!", "–ê–õ–Ø–†–ú üîî, –Ω–æ–≤–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞",
             "–¢–∞–º —ç—Ç–æ, –∫–≤–∞—Ä—Ç–∏—Ä–∞ –Ω–æ–≤–∞—è üëâüèª", "–ù—É —Ç—ã —Ç–∞–º –¥–æ–ª–≥–æ üò∂, –∑–∞–±–µ—Ä—É—Ç –≤–µ–¥—å —Å–∫–æ—Ä–æ"]


async def pars_html():
    service = Service(executable_path="drive/chromedriver")
    # –ù—É–∂–Ω–æ —á—Ç–æ–±—ã –∑–∞–ø—É—Å–∫–∞—Ç—å –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ –±–µ–∑ GUI. –°–∫—Ä–∏–Ω—à–æ—Ç—ã —Ç–∞–∫–∂–µ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")

    # –ù—É–∂–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å, –ø–æ–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –≤—Å—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞, —Ç–∏–ø–∞ Js –∏–ª–∏ –µ—â—ë —á—Ç–æ-—Ç–æ.
    # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–≥–æ, —á—Ç–æ –º–æ–∂–Ω–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π
    chrome_options.page_load_strategy = "eager"

    driver = webdriver.Chrome(options=chrome_options)

    driver.get(url)
    assert "–ö—É–ø–∏—Ç—å" in driver.title

    html = driver.page_source

    try:
        os.mkdir("2avito")
    except FileExistsError:
        pass

    with open(f"2avito/kvartiry_syktyvkar_page_1.html", "w", encoding='utf-8') as file:
        file.write(html)

    with open(f"2avito/kvartiry_syktyvkar_page_1.html", "r", encoding='utf-8') as file:
        page = file.read()

    soup = BeautifulSoup(page, 'html.parser')

    last_page = soup.find_all("span", class_="styles-module-text-InivV")[-1].text

    for i in range(1, int(last_page)):

        try:
            check = driver.find_elements(By.CLASS_NAME, "styles-module-item-kF45w")[-1]
            check.click()

            html = driver.page_source

            print(f"–°–∫–∞—á–∞–ª —Å—Ç—Ä–∞–Ω–∏—Ü—É - {i + 1}")

            with open(f"2avito/kvartiry_syktyvkar_page_{i + 1}.html", "w", encoding='utf-8') as file:
                file.write(html)
        except:
            break

        if i == 3:
            break

    assert "No results found." not in driver.page_source

    driver.close()

    return int(4)


async def get_url(update: Update, context: ContextTypes.DEFAULT_TYPE):
    while True:
        pages = await pars_html()

        time.sleep(3)

        for i in range(0, pages):

            with open(f"2avito/kvartiry_syktyvkar_page_{i+1}.html", "r", encoding='utf-8') as file:
                page = file.read()

            soup = BeautifulSoup(page, 'html.parser')

            urls = []
            for j in soup.find_all("a", class_="iva-item-sliderLink-uLz1v"):
                urls.append(f"https://www.avito.ru{j.get('href')}")

            for j in urls:

                if j not in await check_url_in_db():
                    await append_urls(j)
                    random_text = random.choice(just_text)
                    await context.bot.send_message(chat_id=update.effective_chat.id, text=f"{random_text} - {j}")
                    await asyncio.sleep(3)

                else:
                    pass

        await asyncio.sleep(300)


if __name__ == '__main__':
    asyncio.run(pars_html())
    # asyncio.run(get_url())
